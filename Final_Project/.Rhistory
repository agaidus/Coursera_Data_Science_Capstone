if (len2 > 0) {
for (i in sum(len3, 1):sum(len3, len2)) {
matches[i, 1] <- bigram_dt[bigram_dt[n1 == w1]]$n0[i - len3]
cnt1 <- length(unigram_dt[unigram_dt[n0 == matches[i, 1]]]$freq)
if (cnt1 > 0) freq1 <- unigram_dt[unigram_dt[n0 == matches[i, 1]]]$freq else freq1 <- 0
matches[i, 2] <- bigram_dt[bigram_dt[n1 == w1]]$freq[i - len3] * l2 + freq1 * l3
}
}
unlist(bigram_dt[bigram_dt$n1 == w1]]$n0)[i - len3]
unlist(bigram_dt[bigram_dt$n1 == w1]$n0)[i - len3]
bigram_dt[bigram_dt$n1 == w1]$n0
bigram_dt
head(bigram_dt)
head(bigram_dt)
bigram_dt[bigram_dt$n1 == w1]
bigram_d2$n1
bigram_dt$n1
bigram_dt$n1
w1
trigram_dt$n2
bigram_dt$n1 == w1
bigram_dt[bigram_dt$n1 == w1]
unlist(bigram_dt[bigram_dt$n1 == w1,]$n0)
unlist(bigram_dt[bigram_dt$n1 == w1,]$n0)[i - len3]
cnt1 <- length(unigram_dt[unigram_dt$n0 == matches[i, 1],]$freq)
cnt1
if (len3 > 0) {
for (i in 1:len3) {
matches[i, 1] <- unlist(trigram_dt[trigram_dt$n2 == w2 & trigram_dt$n1 == w1,]$n0)[i]
cnt2 <- length(bigram_dt[bigram_dt$n1 == w1 & bigram_dt$n0 == matches[i, 1],]$freq)
cnt1 <- length(unigram_dt[unigram_dt$n0 == matches[i, 1],]$freq)
if (cnt2 > 0) freq2 <- bigram_dt[bigram_dt$n1 == w1 &
bigram_dt$n0 == matches[i, 1],]$freq else freq2 <- 0
if (cnt1 > 0) freq1 <- unigram_dt[unigram_dt$n0 == matches[i, 1],]$freq else freq1 <- 0
matches[i, 2] <- trigram_dt[trigram_dt$n2 == w2 & trigram_dt$n1 == w1,]$freq[i] *
l1 + freq2 * l2 + freq1 * l3
}
}
if (len2 > 0) {
for (i in sum(len3, 1):sum(len3, len2)) {
matches[i, 1] <- unlist(bigram_dt[bigram_dt$n1 == w1,]$n0)[i - len3]
cnt1 <- length(unigram_dt[unigram_dt$n0 == matches[i, 1],]$freq)
if (cnt1 > 0) freq1 <- unigram_dt[unigram_dt&n0 == matches[i, 1],]$freq else freq1 <- 0
matches[i, 2] <- bigram_dt[bigram_dt$n1 == w1,]$freq[i - len3] * l2 + freq1 * l3
}
}
if (len3 > 0) {
for (i in 1:len3) {
matches[i, 1] <- unlist(trigram_dt[trigram_dt$n2 == w2 & trigram_dt$n1 == w1,]$n0)[i]
cnt2 <- length(bigram_dt[bigram_dt$n1 == w1 & bigram_dt$n0 == matches[i, 1],]$freq)
cnt1 <- length(unigram_dt[unigram_dt$n0 == matches[i, 1],]$freq)
if (cnt2 > 0) freq2 <- bigram_dt[bigram_dt$n1 == w1 &
bigram_dt$n0 == matches[i, 1],]$freq else freq2 <- 0
if (cnt1 > 0) freq1 <- unigram_dt[unigram_dt$n0 == matches[i, 1],]$freq else freq1 <- 0
matches[i, 2] <- trigram_dt[trigram_dt$n2 == w2 & trigram_dt$n1 == w1,]$freq[i] *
l1 + freq2 * l2 + freq1 * l3
}
}
if (len2 > 0) {
for (i in sum(len3, 1):sum(len3, len2)) {
matches[i, 1] <- unlist(bigram_dt[bigram_dt$n1 == w1,]$n0)[i - len3]
cnt1 <- length(unigram_dt[unigram_dt$n0 == matches[i, 1],]$freq)
if (cnt1 > 0) freq1 <- unigram_dt[unigram_dt$n0 == matches[i, 1],]$freq else freq1 <- 0
matches[i, 2] <- bigram_dt[bigram_dt$n1 == w1,]$freq[i - len3] * l2 + freq1 * l3
}
}
matches
matches[which.max(matches[,2])
matches[which.max(matches[,2]
matches[which.max(matches[,2]
matches[which.max(matches[,2]]
matches[which.max(matches[,2])]
input_words
runApp('P:/Documents/DataScience/Capstone/0512')
unigram_dt <- readRDS("P:/Documents/DataScience/Capstone/0512/Data/uni_freq.rds")
bigram_dt <- readRDS("P:/Documents/DataScience/Capstone/0512/Data/bi_freq.rds")
trigram_dt <- readRDS("P:/Documents/DataScience/Capstone/0512/Data/tri_freq.rds")
badwords <- readRDS("P:/Documents/DataScience/Capstone/0512/Data/badwords.rds")
input_words
len <- length(input_words)
if (len > 1) {
w1 <- input_words[len]
w2 <- input_words[len - 1]
} else if (len > 0) {
w1 <- input_words[len]
w2 <- "NA"
} else return("the")
w1
w2
l1 <- .95
l2 <- .04
l3 <- .01
len3 <- length(trigram_dt[trigram_dt$n2 == w2 & trigram_dt$n1 == w1,]$freq)
len3
len2 <- length(bigram_dt[bigram_dt$n1 == w1,]$freq)
len2
matches <- matrix(nrow = len3 + len2, ncol = 2)
matches[,1] <- ""
matches[,2] <- 0
if (len3 > 0) {
for (i in 1:len3) {
matches[i, 1] <- unlist(trigram_dt[trigram_dt$n2 == w2 & trigram_dt$n1 == w1,]$n0)[i]
cnt2 <- length(bigram_dt[bigram_dt$n1 == w1 & bigram_dt$n0 == matches[i, 1],]$freq)
cnt1 <- length(unigram_dt[unigram_dt$n0 == matches[i, 1],]$freq)
if (cnt2 > 0) freq2 <- bigram_dt[bigram_dt$n1 == w1 &
bigram_dt$n0 == matches[i, 1],]$freq else freq2 <- 0
if (cnt1 > 0) freq1 <- unigram_dt[unigram_dt$n0 == matches[i, 1],]$freq else freq1 <- 0
matches[i, 2] <- trigram_dt[trigram_dt$n2 == w2 & trigram_dt$n1 == w1,]$freq[i] *
l1 + freq2 * l2 + freq1 * l3
}
}
matches
if (len2 > 0) {
for (i in sum(len3, 1):sum(len3, len2)) {
matches[i, 1] <- unlist(bigram_dt[bigram_dt$n1 == w1,]$n0)[i - len3]
cnt1 <- length(unigram_dt[unigram_dt$n0 == matches[i, 1],]$freq)
if (cnt1 > 0) freq1 <- unigram_dt[unigram_dt$n0 == matches[i, 1],]$freq else freq1 <- 0
matches[i, 2] <- bigram_dt[bigram_dt$n1 == w1,]$freq[i - len3] * l2 + freq1 * l3
}
}
matches
get_word <- function(text) {
if (text != " ") {
words <- parse_text(tolower(text))
num_words <- length(words)
if (num_words > 0) {
filter <- paste("^", words[num_words], sep = "")
tmp_dt <- unigram_dt[unigram_dt$n0 %like% filter]
pred_word <- dim(tmp_dt)[1]
if (pred_word > 0) {
tmp_dt <- tmp_dt[order(rank(-freq))]
pred <- tmp_dt[1]$n0
if (num_words > 2) {
tmp_w <- paste(words[1])
for (i in 2:(num_words - 1)) tmp_w <- paste(tmp_w, words[i])
return(paste(tmp_w, filter_text(pred)))
} else if (num_words > 1) {
tmp_w <- paste(words[1])
return(paste(tmp_w, filter_text(pred)))
}
}
}
}
return(text)
}
text
words <- parse_text(tolower(text))
words
num_words <- length(words)
filter <- paste("^", words[num_words], sep = "")
filter
tmp_dt <- unigram_dt[unigram_dt$n0 %like% filter]
tmp_dt <- unigram_dt[unigram_dt$n0 %like% filter,]
tmp_dt
pred_word <- dim(tmp_dt)[1]
pred_word
tmp_dt <- tmp_dt[order(rank(-freq))]
tmp_dt <- tmp_dt[tmp_dt$order(rank(-freq)),]
tmp_dt <- tmp_dt[tmp_dt$order(rank(-freq))]
tmp_dt <- tmp_dt[order(rank(-tmp_dt$freq))]
tmp_dt
pred <- tmp_dt[1]$n0
pred
tmp_dt
tmp_dt[1]
tmp_dt[1]$no
tmp_dt$no
tmp_dt[1]$n0
tmp_dt$n0
tmp_dt$n0[1]
pred <- tmp_dt$n0[1]
pred
unlist(pred)
pred <- unlist(tmp_dt$n0[1])
pred
num_word
num_words
tmp_w <- paste(words[1])
tmp_w
words
for (i in 2:(num_words - 1)) tmp_w <- paste(tmp_w, words[i])
return(paste(tmp_w, filter_text(pred)))
for (i in 2:(num_words - 1)) tmp_w <- paste(tmp_w, words[i])
paste(tmp_w, filter_text(pred))
runApp("P:\Documents\DataScience\Capstone\0512")
runApp("P:/Documents/DataScience/Capstone/0512")
get_word('hello my name')
get_word('hello my name is')
text='hello my name is'
words <- parse_text(tolower(text))
words
num_words <- length(words)
filter <- paste("^", words[num_words], sep = "")
tmp_dt <- unigram_dt[unigram_dt$n0 %like% filter,]
pred_word <- dim(tmp_dt)[1]
pred_word
tmp_dt
tmp_dt <- tmp_dt[order(rank(-tmp_dt$freq))]
tmp_dt <- tmp_dt[order(-tmp_dt$freq)]
tmp_dt <- tmp_dt[order(rank(-tmp_dt$freq)),]
tmp_dt
pred <- unlist(tmp_dt$n0[1])
pred
get_word <- function(text) {
if (text != " ") {
words <- parse_text(tolower(text))
num_words <- length(words)
if (num_words > 0) {
filter <- paste("^", words[num_words], sep = "")
tmp_dt <- unigram_dt[unigram_dt$n0 %like% filter,]
pred_word <- dim(tmp_dt)[1]
if (pred_word > 0) {
tmp_dt <- tmp_dt[order(rank(-tmp_dt$freq)),]
pred <- unlist(tmp_dt$n0[1])
if (num_words > 2) {
tmp_w <- paste(words[1])
for (i in 2:(num_words - 1)) tmp_w <- paste(tmp_w, words[i])
return(paste(tmp_w, filter_text(pred)))
} else if (num_words > 1) {
tmp_w <- paste(words[1])
return(paste(tmp_w, filter_text(pred)))
}
}
}
}
return(text)
}
get_word('hello my name is')
unigram_dt[unigram_dt$n0 %like% filter,]
unigram_dt[unigram_dt$n0 == filter,]
unigram_dt[unigram_dt$n0 == 'istanbul',]
unigram_dt[unigram_dt$n0 == 'is',]
unigram_dt[unigram_dt$n0 == 'the',]
unigram_dt[unigram_dt$n0 == 'a',]
unigram_dt[unigram_dt$n0 == 'to',]
runApp("P:/Documents/DataScience/Capstone/0512")
#https://github.com/dhashman/datasciencecapstone
library(doParallel)
#registerDoParallel(makeCluster(4))
library(stringr)
library(dplyr)
library(caret)
library(tau)
library(data.table)
library(RWeka)
library(tm)
library(xtable)
library(slam)
unzipped_folder <- "P:/Documents/DataScience/Capstone/Data/Coursera-SwiftKey/final/en_US/"
unzipped_blogs_file <- paste(unzipped_folder, "en_US.blogs.txt", sep = "")
unzipped_twitter_file <- paste(unzipped_folder, "en_US.twitter.txt", sep = "")
unzipped_news_file <- paste(unzipped_folder, "en_US.news.txt", sep = "")
#rm(unzipped_folder)
fileMunge<- function(x) {
text<-readLines(x, n=500000)
totalLines=length(text)
chunkSize=25000
chunks=totalLines/chunkSize
remainder = chunks %% 1
wholeChunks = chunks-remainder
# initialize list
output=list()
# break file into chunks
i=1
line=1
while (i<=wholeChunks){
end=line+chunkSize-1
output[[i]]<-text[line:end]
line=end+1
i=i+1
}
output[[i]]<-text[line:totalLines]
# Text Transformations to remove odd characters #
output=lapply(output,FUN=iconv, to='ASCII', sub=' ')
output=lapply(output,FUN= function(x) gsub("'{2}", " ",x))
output=lapply(output,FUN= function(x) gsub("[0-9]", " ",x))
}
text=fileMunge(unzipped_news_file)
# Make Corpus and do transformations
makeCorpus<- function(x) {
corpus<-Corpus(VectorSource(x))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(tolower))
# corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
corpus<- tm_map(corpus,removePunctuation)
corpus<- tm_map(corpus,removeNumbers)
return(corpus)
}
corp=makeCorpus(text)
corp
uni_tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
bi_tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tri_tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
quad_tokenizer<- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
uni_matrix <- TermDocumentMatrix(corp, control = list(tokenize = uni_tokenizer))
uni_freq=data.frame(grams=names(row_sums(uni_matrix)), counts=row_sums(uni_matrix))
uni_freq=uni_freq[which(uni_freq$counts>=5),]
uni_freq$freq=uni_freq$counts/sum(uni_freq$count)
uni_freq$words<-lapply(uni_freq$grams,FUN=function(x){unlist(strsplit(as.character(x),"\\s+"))})
uni_freq$n0=lapply(uni_freq$words,FUN=function(x){x[1]})
uni_freq
head(uniq_frqe)
head(uniq_freq
)
head(uni_freq)
uni_freq[uniq_freq$words=='is']
uni_freq[uni_freq$words=='is']
uni_freq[uni_freq$words=='is']
head(bi_freq)
?stemDocument
# Make Corpus and do transformations
makeCorpus<- function(x) {
corpus<-Corpus(VectorSource(x))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(tolower))
# corpus <- tm_map(corpus, removeWords, stopwords("english"))
#corpus <- tm_map(corpus, stemDocument)
corpus<- tm_map(corpus,removePunctuation)
corpus<- tm_map(corpus,removeNumbers)
return(corpus)
}
corp=makeCorpus(text)
uni_tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
bi_tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tri_tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
quad_tokenizer<- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
uni_matrix <- TermDocumentMatrix(corp, control = list(tokenize = uni_tokenizer))
uni_freq=data.frame(grams=names(row_sums(uni_matrix)), counts=row_sums(uni_matrix))
head(uni_freq)
uni_freq[uniq_freq$grams=='is']
uni_freq[uni_freq$grams=='is']
uni_freq[uni_freq$grams=='aaa']
head(uni_freq)
uni_freq[uni_freq$grams=='aaaaambivalence']
uni_freq[uni_freq$grams=='aaaaambivalence',]
uni_freq[uni_freq$grams=='aaa',]
uni_freq[uni_freq$grams=='is',]
head(text)
x=text
x
corp
head(bi_freq)
uni_freq
uni_freq[uni_freq$grams=='a',]
uni_matrix <- TermDocumentMatrix(corp, control = list(tokenize = uni_tokenizer))
uni_freq=data.frame(grams=names(row_sums(uni_matrix)), counts=row_sums(uni_matrix))
uni_freq=uni_freq[which(uni_freq$counts>=5),]
uni_freq$freq=uni_freq$counts/sum(uni_freq$count)
uni_freq$words<-lapply(uni_freq$grams,FUN=function(x){unlist(strsplit(as.character(x),"\\s+"))})
uni_freq$n0=lapply(uni_freq$words,FUN=function(x){x[1]})
bi_matrix <- TermDocumentMatrix(corp, control = list(tokenize = bi_tokenizer))
bi_freq=data.frame(grams=names(row_sums(bi_matrix)),counts=row_sums(bi_matrix))
bi_freq=bi_freq[which(bi_freq$counts>=5),]
bi_freq$words<-lapply(bi_freq$grams,FUN=function(x){unlist(strsplit(as.character(x),"\\s+"))})
bi_freq$query<-lapply(bi_freq$words,FUN=function(x){x[1]})
bi_freq$result<-lapply(bi_freq$words,FUN=function(x){x[2]})
bi_freq$freq=bi_freq$counts/sum(bi_freq$count)
bi_freq$n1=lapply(bi_freq$words,FUN=function(x){x[1]})
bi_freq$n0=lapply(bi_freq$words,FUN=function(x){x[2]})
uni_freq[uni_freq$grams=='a',]
head(uni_freq)
head(bi_freq)
bi_freq[bi_freq$n1=='is']
bi_freq[bi_freq$n1=='is',]
head(bi_freq[bi_freq$n1=='is',])
head(bi_freq[bi_freq$n0=='is',])
uni_freq=data.frame(grams=names(row_sums(uni_matrix)), counts=row_sums(uni_matrix))
uni_freq=uni_freq[which(uni_freq$counts>=5),]
uni_freq$freq=uni_freq$counts/sum(uni_freq$count)
head(uni_freq)
uni_matrix <- TermDocumentMatrix(corp, control = list(tokenize = uni_tokenizer))
uni_matrix
uni_freq=data.frame(grams=names(row_sums(uni_matrix)), counts=row_sums(uni_matrix))
head(uni_freq)
uni_freq[uni_freq$grams=='name',]
uni_freq[uni_freq$grams=='is',]
uni_freq[uni_freq$grams==' is',]
uni_freq[uni_freq$grams=='the',]
corp
a=corp[[1]]$content
len(a)
lenght(a)
length(a)
uni_matrix
length(uni_matrix)
a=corp[1]$content
len(a)
lenght(a)
length(a)
a
a=corp[[1]]$content
length(a)
a
a
length(text)
a=corp[[2]]$content
length(a)
a=corp[[4]]$content
length(a)
base1 <- textcnt(a, method = "string", split = "[[:space:]]", n = 1L, decreasing = T)
base1
head(base1)
runApp('P:/Documents/DataScience/Capstone/0512')
runApp('P:/Documents/DataScience/Capstone/0512')
runApp('P:/Documents/DataScience/Capstone/0512')
runApp('P:/Documents/DataScience/Capstone/0512')
runApp('P:/Documents/DataScience/Capstone/0512')
runApp('P:/Documents/DataScience/Capstone/0512')
runApp('P:/Documents/DataScience/Capstone/0512')
runApp('P:/Documents/AG/519Test/TheNextWord')
library(shiny)
runApp('P:/Documents/AG/519Test/TheNextWord')
runApp('P:/Documents/AG/519Test/TheNextWord')
runApp('P:/Documents/AG/519Test/TheNextWord')
runApp('P:/Documents/AG/519Test/TheNextWord')
runApp('P:/Documents/AG/519Test/TheNextWord')
runApp('P:/Documents/AG/519Test/TheNextWord')
runApp('P:/Documents/AG/519Test/TheNextWord')
runApp('P:/Documents/AG/519Test/TheNextWord')
runApp('P:/Documents/AG/519Test/TheNextWord')
NextGuess<-NgramsTable$word2[2]
NgramsTable <<- Dataset[list("-", TextInputA[1])]
TextInputA='yes'
NgramsTable <<- Dataset[list("-", TextInputA[1])]
NgramsTable <<- NgramsTable[NgramsTable$word2!="-", ]
NgramsTable <<- NgramsTable[order(NgramsTable$freq, decreasing=TRUE), ]
NgramsTable
if (length(NgramsTable)>1){print('yes')}
NextGuess=''
if (length(NgramsTable)>1){NextGuess<-NgramsTable$word2[2]}
NextGuess
runApp('P:/Documents/AG/519Test/TheNextWord')
runApp('P:/Documents/AG/519Test/TheNextWord')
runApp('P:/Documents/AG/519Test/TheNextWord')
TextInputB=Split_Translate_Input('become the')
TextInputB
Word_Count1 <- function(TextInputA){
NgramsTable <<- Dataset[list("-", TextInputA[1])]
NgramsTable <<- NgramsTable[NgramsTable$word2!="-", ]
NgramsTable <<- NgramsTable[order(NgramsTable$freq, decreasing=TRUE), ]
#Next Guess
NextGuess=''
if (length(NgramsTable)>1){NextGuess<-NgramsTable$word2[2]}
#NextGuess<-NgramsTable$word2[2]
if(is.na(NextGuess)|is.null(NextGuess)){NextGuess <- ''}
Guess_Output <- NgramsTable$word2[1]
if(is.na(Guess_Output)|is.null(Guess_Output)){
Guess_Output <- "Uh oh! I have no prediction for you!"
}
#return(Guess_Output)
return (c(Guess_Output,NextGuess))
}
Word_Count1(TextInputB)
TextInputB
WOrkd_Count1(TextInputA)
Word_Count2 <- function(TextInputB){
NgramsTable <<- Dataset[list("-", TextInputB[1], TextInputB[2])]
NgramsTable <<- NgramsTable[NgramsTable$word3!="-", ]
NgramsTable <<- NgramsTable[order(NgramsTable$freq, decreasing=TRUE), ]
#Next Guess
NextGuess=''
if (length(NgramsTable)>1){NextGuess<-NgramsTable$word3[2]}
#NextGuess<-NgramsTable$word3[2]
if(is.na(NextGuess)|is.null(NextGuess)){NextGuess <- '-'}
Guess_Output <- NgramsTable$word3[1]
if(is.na(Guess_Output)|is.null(Guess_Output)){
Guess_Output <- Word_Count1(TextInputB[2])
}
#return(Guess_Output)
return (c(Guess_Output,NextGuess))
}
Word_Count2(TextInputB)
NgramsTable <<- Dataset[list("-", TextInputB[1], TextInputB[2])]
NgramsTable <<- NgramsTable[NgramsTable$word3!="-", ]
NgramsTable <<- NgramsTable[order(NgramsTable$freq, decreasing=TRUE), ]
NgramsTable
NgramsTable[c('word1','word2','word3')]
NgramsTable
NgramsTable[c(1,2)]
NgramsTable[,c('word1','word2','word3')]
NgramsTable
subset(NgramsTable, select=c('word1','word2','word3'))
duplicated(subset(NgramsTable, select=c('word1','word2','word3')),)
dups=duplicated(subset(NgramsTable, select=c('word1','word2','word3')),)
NgramsTable[!dups, ]
runApp('P:/Documents/AG/519Test/TheNextWord')
runApp('P:/Documents/AG/519Test/TheNextWord')
runApp('P:/Documents/AG/519Test/TheNextWord')
runApp('P:/Documents/AG/519Test/TheNextWord')
runApp('P:/Documents/AG/519Test/TheNextWord')
runApp('P:/Documents/AG/519Test/TheNextWord')
runApp('P:/Documents/AG/519Test/TheNextWord')
library(shiny)
library(data.table)
library(NLP)
library(tm)
install.packages('shiny')
install.packages('data.table')
install.packages("NLP")
install.packages("tm")
runApp('P:/Documents/DataScience/Capstone/Coursera_DataScience_Capstone/Final')
libary(shiny)
library(shiny)
runApp('P:/Documents/DataScience/Capstone/Coursera_DataScience_Capstone/Final')
library(shiny)
library(data.table)
library(NLP)
library(tm)
install.packages('data.table')
library(data.table)
runApp('P:/Documents/DataScience/Capstone/Coursera_DataScience_Capstone/Final')
